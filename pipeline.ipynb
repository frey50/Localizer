{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6ApCc5YmY0u26vDdu+zRA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frey50/Localizer/blob/main/pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ek8-yfRG8By4"
      },
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────\n",
        "#  🔁 NLLB 1.8B + OpenChat Mistral 7B Pipeline\n",
        "#      Uzbek (uzn_Latn) ⇄ English (eng_Latn)\n",
        "#      Full system: Translation + Chatbot + Back Translation\n",
        "#      Quantized models, no Fast tokenizer, ready to run.\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "\n",
        "# ───── INSTALL DEPENDENCIES (if needed) ─────\n",
        "# !pip install -U transformers sentencepiece bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    BitsAndBytesConfig,\n",
        "    NllbTokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "dW-BA7I-8Kju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ───── 1. DEVICE + CONFIG ─────\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ───── 2. LOAD TRANSLATOR (NLLB 1.8B, 4-bit) ─────\n",
        "trans_model_id = \"facebook/nllb-200-1.3B\"  # Note: no official 1.8B, this is closest size\n",
        "bnb_cfg = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "E71jvmXs8Kqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trans_tokenizer = NllbTokenizer.from_pretrained(trans_model_id)  # slow tokenizer\n",
        "trans_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    trans_model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_cfg\n",
        ")"
      ],
      "metadata": {
        "id": "bIXF1SZv8Ksj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ───── 3. LOAD CHATBOT (OpenChat Mistral 7B) ─────\n",
        "chat_model_id = \"openchat/openchat-3.5-1210\"\n",
        "chat_tokenizer = AutoTokenizer.from_pretrained(chat_model_id, trust_remote_code=True)\n",
        "chat_model = AutoModelForCausalLM.from_pretrained(\n",
        "    chat_model_id,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config\n",
        ")\n"
      ],
      "metadata": {
        "id": "4CicksWO8Xky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ───── 4. LANG CODES ─────\n",
        "UZ = \"uzn_Latn\"\n",
        "EN = \"eng_Latn\"\n",
        "\n",
        "# ───── 5. TRANSLATION FUNCTION ─────\n",
        "def translate(text, src_lang, tgt_lang, prompt=\"\"):\n",
        "    trans_tokenizer.src_lang = src_lang\n",
        "    input_text = (prompt + \" \" + text).strip()\n",
        "    inputs = trans_tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "    bos_id = trans_tokenizer.convert_tokens_to_ids(tgt_lang)\n",
        "    outputs = trans_model.generate(\n",
        "        **inputs,\n",
        "        forced_bos_token_id=bos_id,\n",
        "        max_length=256,\n",
        "        num_beams=4\n",
        "    )\n",
        "    return trans_tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n"
      ],
      "metadata": {
        "id": "_htHrxoF8a_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ───── 6. OPENCHAT RESPONSE ─────\n",
        "def chat_with_openchat(prompt):\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ]\n",
        "    tokens = chat_tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n",
        "    out_ids = chat_model.generate(tokens, max_new_tokens=256, do_sample=True)\n",
        "    out_text = chat_tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
        "    return out_text.split(\"<|assistant|>\")[-1].strip()\n"
      ],
      "metadata": {
        "id": "6bSpiOuR8fB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ───── 7. FULL PIPELINE ─────\n",
        "def uzbek_chatbot_pipeline(user_uz_text):\n",
        "    # print(f\"🧠 Input (Uzbek): {user_uz_text}\")\n",
        "    en_input = translate(user_uz_text, UZ, EN)\n",
        "    # print(f\"➡️ Translated to English: {en_input}\")\n",
        "\n",
        "    en_output = chat_with_openchat(en_input)\n",
        "    # print(f\"💬 LLM Output (English): {en_output}\")\n",
        "\n",
        "    uz_output = translate(en_output, EN, UZ)\n",
        "    print(f\"🔁 Back to Uzbek: {uz_output}\")\n",
        "    return uz_output\n"
      ],
      "metadata": {
        "id": "KsRIJMdY8bCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ───── 8. TEST LOOP ─────\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🌐 Uzbek-English Chatbot | Type 'q' to quit\\n\")\n",
        "    while True:\n",
        "        text = input(\"👤 > \")\n",
        "        if text.lower().strip() == 'q':\n",
        "            break\n",
        "        uzbek_chatbot_pipeline(text)\n",
        "        print(\"\\n\")"
      ],
      "metadata": {
        "id": "DaKBuBQi8bDr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
